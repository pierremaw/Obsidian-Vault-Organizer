{"path":".obsidian/plugins/text-extractor/cache/tr/_attachments-Transformer Architecture.png-75bcf23ac21a48623256ee26834ca2c2.json","text":"Output Probabilities Add & Norm Feed Fovard M- Head Feed Attention Forward 7 J Nx N Add & Norm == Muiti-Head Muiti-Head ‘Atention Atertion —-—— 1 J Positional Positional Ercoirg QO = Ercoing Tt Guipat Embedding Embedding Inputs Outputs (shifted right) Figure 1: The Transformer - model architecture.","libVersion":"0.0.0","langs":"eng","hash":"","size":0}